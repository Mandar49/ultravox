# SLM with ultravox & llama3.1, trained wtih knowledge distillation.
exp_name: "ultravox-v0_4"

# Make sure to accept the license agreement on huggingface hub
text_model: "meta-llama/Meta-Llama-3.1-8B-Instruct"
audio_model: "openai/whisper-medium"

loss_config:
  # Choose from ["KL_Divergence", "CrossEntropy"], default is "KL_Divergence"
  loss_function: "KL_Divergence"

train_sets:
  - name: librispeech-clean-continuation
  - name: librispeech-other-continuation
  - name: peoplespeech-continuation
    weight: 8
  - name: common-voice-en-continuation
    weight: 8
  - name: common-voice-ar-continuation
    weight: 0.2
  - name: common-voice-de-continuation
    weight: 4
  - name: common-voice-es-continuation
    weight: 3
  - name: common-voice-fr-continuation
    weight: 4
  - name: common-voice-it-continuation
    weight: 1.2
  - name: common-voice-ja-continuation
    weight: 0.1
  - name: common-voice-pt-continuation
    weight: 0.2
  - name: common-voice-ru-continuation
    weight: 0.2
  - name: librispeech-clean-transcription
  - name: librispeech-other-transcription
  - name: peoplespeech-transcription
    weight: 0.8
  - name: common-voice-en-transcription
    weight: 0.8
  - name: common-voice-ar-transcription
    weight: 0.02
  - name: common-voice-de-transcription
    weight: 0.4
  - name: common-voice-es-transcription
    weight: 0.3
  - name: common-voice-fr-transcription
    weight: 0.4
  - name: common-voice-it-transcription
    weight: 0.12
  - name: common-voice-ja-transcription
    weight: 0.01
  - name: common-voice-pt-transcription
    weight: 0.02
  - name: common-voice-ru-transcription
    weight: 0.02

# Temporarily remove heysquad_human from val_sets as it causes the training to fail.
val_sets: ["peoplespeech"]

batch_size: 24
max_steps: 14400 # x8x24 = 2,764,800
