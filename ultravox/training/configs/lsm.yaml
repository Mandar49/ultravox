exp_name: 'lsm_librilight_podcasts'

model_type: 'lsm'
text_model: 'meta-llama/Llama-3.2-1B'

model_load_dir: wandb://fixie/ultravox/model-1b_to_8b_directnew_proj_lv3t-merge-lora_bs12_lrdiv4:v5

train_sets:
  - name: libri_light_medium
  - name: podcast_dialogue

val_sets:
  - name: podcast_dialogue

do_eval: False

batch_size: 8
expected_audio_length_seconds: 30

max_steps: 40000
val_steps: 0.1

lr:  5.e-4

text_model_lora_config:
  unfreeze_layers: ['.*']

train_dataset_args:
  # This might be confusing, but we have two ways filtering samples. The first one was for Ultravox and would filter out the whole
  # sample if its audio is longer than X seconds. We don't want that.
  # ultravoxls_data_proc takes care of splitting long samples into multiple short samples
  max_audio_duration_secs: -1  # -1 means no filtering
  # since the samples are too big, we need to reduce the shuffle buffer size
  # otherwise it'll take too long to start the training and we timeout
  shuffle_buffer_size: 50

val_dataset_args:
  max_audio_duration_secs: -1  # -1 means no filtering

loss_config:
  initial_tokens_to_ignore: 10

max_steps: 100000
lr_warmup_steps: 0.1  # target 10% of max_steps

num_workers: 18